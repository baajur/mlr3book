# Use Cases {#use-cases}

This chapter is a collection of use cases to showcase `r mlr_pkg("mlr3")`.
The first use case shows different functions, using [house price data](#use-case-regr-houses), housing price data in King Country.

Following features are illustrated:

* Summarizing the data set
* Converting data to treat it as a numeric feature/factor
* Generating new variables
* Splitting data into train and test data sets
* Computing a first model (decision tree)
* Building many trees (random forest)
* Visualizing price data across different region
* Optimizing the baseline by implementing a tuner
* Engineering features
* Creating a sparser model

Further use cases are following soon!


## House Price Prediction in King County {#use-case-regr-houses}

We use the `house_sales_prediction` dataset contained in this book in order to provide a use-case for the application of `mlr3` on real-world data.

```{r 09-use-cases-001}
library(mlr3book)
data("house_sales_prediction", package = "mlr3book")
```

### Exploratory Data Analysis

In order to get a quick impression of our data, we perform some initial *Exploratory Data Analysis*.
This helps us to get a first impression of our data and might help us arrive at additional features that can help with the prediction of the house prices.

We can get a quick overview using R's summary function:

```{r 09-use-cases-002}
summary(house_sales_prediction)
dim(house_sales_prediction)
```

Our dataset has `r nrow(house_sales_prediction)` observations and `r ncol(house_sales_prediction)` columns.
The variable we want to predict is `price`.
In addition to the price column, we have several other columns:

* `id:` A unique identifier for every house.

* `date`: A date column, indicating when the house was sold.
  This column is currently not encoded as a `date` and requires some preprocessing.

* `zipcode`: A column indicating the ZIP code. This is a categorical variable with many factor levels.

* `long, lat` The longitude and latitude of the house

* `...` several other numeric columns providing information about the house, such as number of rooms, square feet etc.

Before we continue with the analysis,  we preprocess some features so that they are stored in the correct format.

First we convert the `date` column to `numeric` to be able to treat it as a numeric feature:

```{r 09-use-cases-003, message = FALSE}
library(lubridate)
house_sales_prediction$date = ymd(substr(house_sales_prediction$date, 1, 8))
house_sales_prediction$date = as.numeric(as.Date(house_sales_prediction$date, origin = "1900-01-01"))
house_sales_prediction$date = house_sales_prediction$date
```

Afterwards, we convert the zip code to a factor:

```{r 09-use-cases-004}
house_sales_prediction$zipcode = as.factor(house_sales_prediction$zipcode)
```

And add a new column **renovated** indicating whether a house was renovated at some point.

```{r 09-use-cases-005}
house_sales_prediction$renovated = as.numeric(house_sales_prediction$yr_renovated > 0)
# And drop the id column:
house_sales_prediction$id = NULL
```

Additionally we convert the price from Dollar to units of 1000 Dollar to improve readability.

```{r 09-use-cases-006}
house_sales_prediction$price = house_sales_prediction$price / 1000
```

We can now plot the density of the **price** to get a first impression on its distribution.

```{r 09-use-cases-007}
library(ggplot2)
ggplot(house_sales_prediction, aes(x = price)) + geom_density()
```

We can see that the prices for most houses lie between 75.000 and 1.5 million dollars.
There are few extreme values of up to 7.7 million dollars.

Feature engineering often allows us to incorporate additional knowledge about the data and underlying processes.
This can often greatly enhance predictive performance.
A simple example: A house which has `yr_renovated == 0` means that is has not been renovated yet.
Additionally we want to drop features which should not have any influence (`id column`).

After those initial manipulations, we load all required packages and create a Task containing our data.

```{r 09-use-cases-008}
library(mlr3)
library(mlr3viz)
tsk = TaskRegr$new("sales", house_sales_prediction, target = "price")
```

We can inspect associations between variables using `mlr3viz`'s `autoplot` function in order to get some good first impressions for our data.
Note, that this does in no way prevent us from using other powerful plot functions of our choice on the original data.

#### Distribution of the price:

The outcome we want to predict is the **price** variable.
The `autoplot` function provides a good first glimpse on our data.
As the resulting object is a `ggplot2` object, we can use `faceting` and other functions from **ggplot2** in order to enhance plots.

```{r 09-use-cases-009}
library(ggplot2)
autoplot(tsk) + facet_wrap(~renovated)
```

We can observe that renovated flats seem to achieve higher sales values, and this might thus be a relevant feature.

Additionally, we can for example look at the condition of the house.
Again, we clearly can see that the price rises with increasing condition.

```{r 09-use-cases-010}
autoplot(tsk) + facet_wrap(~condition)
```

#### Association between variables

In addition to the association with the target variable, the association between the features can also lead to interesting insights.
We investigate using variables associated with the quality and size of the house.
Note that we use `$clone()` and `$select()` to clone the task and select only a subset of the features for the `autoplot` function, as `autoplot` per default uses all features.
The task is cloned before we select features in order to keep the original task intact.

```{r 09-use-cases-011}
# Variables associated with quality
autoplot(tsk$clone()$select(tsk$feature_names[c(3, 17)]),
  type = "pairs")
```

```{r 09-use-cases-012}
autoplot(tsk$clone()$select(tsk$feature_names[c(9:12)]),
  type = "pairs")
```

### Splitting into train and test data

In `mlr3`, we do not create `train` and `test` data sets, but instead keep only a vector of train and test indices.

```{r 09-use-cases-013}
set.seed(4411)
train.idx = sample(seq_len(tsk$nrow), 0.7 * tsk$nrow)
test.idx = setdiff(seq_len(tsk$nrow), train.idx)
```

### A first model: Decision Tree

Decision trees cannot only be used as a powerful tool for predictive models but also for exploratory data analysis.
In order to fit a decision tree, we first get the `regr.rpart` learner from the `mlr_learners` dictionary by using the sugar function `r ref("lrn")`.

For now we leave out the  `zipcode` variable, as we also have the `latitude` and `longitude` of each house.

```{r 09-use-cases-014}
tsk_nozip = tsk$clone()$select(setdiff(tsk$feature_names, "zipcode"))
# Get the learner
lrn = lrn("regr.rpart")
# And train on the task
lrn$train(tsk_nozip, row_ids = train.idx)
```

```{r 09-use-cases-015, width = 10, height = 10}
plot(lrn$model)
text(lrn$model)
```

The learned tree relies on several variables in order to distinguish between cheaper and pricier houses.
The features we split along are **grade**, **sqft_living**, but also some features related to the area (longitude and latitude).

We can visualize the price across different regions in order to get more info:

```{r 09-use-cases-016, message = FALSE}
# Load the ggmap package in order to visualize on a map
library(ggmap)

# And create a quick plot for the price
qmplot(long, lat, maptype = "watercolor", color = log(price),
  data = house_sales_prediction[train.idx[1:3000],]) +
  scale_colour_viridis_c()

# And the zipcode
qmplot(long, lat, maptype = "watercolor", color = zipcode,
  data = house_sales_prediction[train.idx[1:3000],]) + guides(color = FALSE)
```

We can see that the price is clearly associated with the zipcode when comparing then two plots.
As a result, we might want to indeed use the **zipcode** column in our future endeavours.

### A first baseline: Decision Tree

After getting an initial idea for our data, we might want to construct a first baseline, in order to see what a simple model already can achieve.

We use `resample` with `3-fold cross-validation` on our training data in order to get a reliable estimate of the algorithm's performance on future data.
Before we start with defining and training learners, we create a `r ref("Resampling")` in order to make sure that we always compare on exactly the same data.

```{r 09-use-cases-017}
library(mlr3learners)
cv3 = rsmp("cv", folds = 3)
cv3$instantiate(tsk$clone()$filter(train.idx))
```

For the cross-validation we only use the **training data** by cloning the task and selecting only observations from the training set.

```{r 09-use-cases-018}
lrn_rpart = lrn("regr.rpart")
res = resample(task = tsk$clone()$filter(train.idx), lrn_rpart, cv3)
res$score(msr("regr.mse"))
sprintf("RMSE of the simple rpart: %s", round(sqrt(res$aggregate()), 2))
```

### Many Trees: Random Forest

We might be able to improve upon the **RMSE** using more powerful learners.
We first load the `mlr3learners` package, which contains the **ranger** learner (a package which implements the "Random Forest" algorithm).

```{r 09-use-cases-019}
lrn_ranger = lrn("regr.ranger", num.trees = 15L)
res = resample(task = tsk$clone()$filter(train.idx), lrn_ranger, cv3)
res$score(msr("regr.mse"))
sprintf("RMSE of the simple ranger: %s", round(sqrt(res$aggregate()), 2))
```

Often tuning **RandomForest** methods does not increase predictive performances substantially.
If time permits, it can nonetheless lead to improvements and should thus be performed.
In this case, we resort to tune a different kind of model: **Gradient Boosted Decision Trees** from the package `r cran_pkg("xgboost")`.

### A better baseline: `AutoTuner`

Tuning can often further improve the performance.
In this case, we *tune* the xgboost learner in order to see whether this can improve performance.
For the `AutoTuner` we have to specify a **Termination Criterion** (how long the tuning should run) a **Tuner** (which tuning method to use) and a **ParamSet** (which space we might want to search through).
For now we do not use the **zipcode** column, as `r cran_pkg("xgboost")` cannot naturally
deal with categorical features.
The **AutoTuner** automatically performs nested cross-validation.

```{r 09-use-cases-020, eval = FALSE}
set.seed(444L)
library(mlr3tuning)
library(paradox)
lrn_xgb = lrn("regr.xgboost")

# Define the ParamSet
ps = ParamSet$new(
  params = list(
    ParamDbl$new(id = "eta", lower = 0.2, upper = .4),
    ParamDbl$new(id = "min_child_weight", lower = 1, upper = 20),
    ParamDbl$new(id = "subsample", lower = .7, upper = .8),
    ParamDbl$new(id = "colsample_bytree",  lower = .9, upper = 1),
    ParamDbl$new(id = "colsample_bylevel", lower = .5, upper = .7),
    ParamInt$new(id = "nrounds", lower = 1L, upper = 25)
))

# Define the Terminator
terminator = TerminatorEvaluations$new(10)
cv3 = rsmp("cv", folds = 3)
at = AutoTuner$new(lrn_xgb, cv3, measures = msr("regr.mse"), ps,
  terminator, tuner = TunerRandomSearch, tuner_settings = list())
```

```{r 09-use-cases-021, echo = FALSE, results = 'hide', eval = FALSE}
# And resample the AutoTuner
res = resample(tsk_nozip$clone()$filter(train.idx), at, cv3)
```

```{r 09-use-cases-022, eval = FALSE}
res$score(msr("regr.mse"))
sprintf("RMSE of the tuned xgboost: %s", round(sqrt(res$aggregate()), 2))
```

We can obtain the resulting params in the respective splits by accessing the `r ref("ResampleResult")`.

```{r 09-use-cases-023}
sapply(res$learners, function(x) x$param_set$values)
```

**NOTE:** To keep runtime low, we only tune parts of the hyperparameter space of `r cran_pkg("xgboost")` in this example.
Additionally, we only allow for $10$ random search iterations, which is usually to little for real-world applications.
Nonetheless, we are able to obtain an improved performance when comparing to the `r cran_pkg("ranger")` model.

In order to further improve our results we have several options:

* Find or engineer better features
* Remove Features to avoid overfitting
* Obtain additional data (often prohibitive)
* Try more models
* Improve the tuning
   * Increase the tuning budget
   * Enlarge the tuning search space
   * Use a more efficient tuning algorithm
* Stacking and Ensembles (see [Pipelines](#pipelines))

Below we will investigate some of those possibilities and investigate whether this improves performance.

### Engineering Features: Mutating ZIP-Codes

In order to better cluster the zip codes, we compute a new feature: **med_price**:
It computes the median price in each zip-code.
This might help our model to improve the prediction.

```{r 09-use-cases-024}
# Create a new feature and append it to the task
zip_price = house_sales_prediction[, .(med_price = median(price)), by = zipcode]

# Join on the original data to match with original columns, then cbind to the task
tsk$cbind(house_sales_prediction[zip_price, on="zipcode"][,"med_price"])
```

Again, we run `resample` and compute the **RMSE**.

```{r 09-use-cases-025}
lrn_ranger = lrn("regr.ranger")
res = resample(task = tsk$clone()$filter(train.idx), lrn_ranger, cv3)
res$score(msr("regr.mse"))
sprintf("RMSE of ranger with med_price: %s", round(sqrt(res$aggregate()), 2))
```

### Obtaining a sparser model

In many cases, we might want to have a sparse model.
For this purpose we can use a `r ref("mlr3filters::Filter")` implemented in `r mlr_pkg("mlr3filters")`.
This can prevent our learner from overfitting make it easier for humans to interpret models as fewer variables influence the resulting prediction.

```{r 09-use-cases-026}
library(mlr3filters)
filter = FilterMRMR$new()$calculate(tsk)
tsk_ftsel = tsk$clone()$select(head(names(filter$scores), 12))
```

The resulting **RMSE** is slightly higher, and at the same time we only use $12$ features.

```{r 09-use-cases-027}
lrn_ranger = lrn("regr.ranger")
res = resample(task = tsk_ftsel$clone()$filter(train.idx), lrn_ranger, cv3)
res$score(msr("regr.mse"))
sprintf("RMSE of ranger with filtering: %s", round(sqrt(res$aggregate()), 2))
```



## Credit worthiness prediction {#use-case-german-credit1}

We use the `german_credit` dataset (contained in this book) for our next example.
The German credit data is a research data set of the University of Hamburg from 1994 donated by Prof. Hans Hoffman.
The data features personal, demographic and financial data on individuals.
Furthermore, it contains an assessment on the individual's creditworthniness.
A detailed description (and manual download) can be found at the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).

As the `"GermanCredit"` data set is frequently used within `mlr3`, there is a task already existing just for this purpose.

```{r 09-use-cases-2-001}
task = tsk("german_credit")
```

Our use case is primarily concerned with representing the complete `mlr3` workbench.
Our special focus is on the parametrisation of learners.

### Exploratory Data Analysis

We can get a quick overview on our data set using R's summary function:

```{r 09-use-cases-2-002}
german_credit = task$data()
summary(german_credit)
dim(german_credit)
```

Our dataset has `r nrow(german_credit)` observations and `r ncol(german_credit)` columns.
The variable we want to predict is `credit_risk` (either **good** or **bad**).
That means we aim to classify people by their credit risk (**good** or **bad**)
In total we have access to the following `r ncol(german_credit)` features:

  * `checking_status:          status/balance of checking account at this bank`
  * `duration:                 duration of the credit in months`
  * `credit_history:           past credit history of applicant at this bank`
  * `purpose:                  reason customer is applying for a loan`
  * `credit_amount:            amount asked by applicant`
  * `savings_status:           savings accounts/bonds at this bank`
  * `employment:               present employment since`
  * `installment_commitment:   installment rate in percentage of disposable income`
  * `personal_status:          combination of sex and personal status of applicant`
  * `other_parties:            other debtors/guarantors present?`
  * `residence_since:          present residence since`
  * `property_magnitude:       properties that applicant has`
  * `age:                      age in years`
  * `other_payment_plans:      other installment plans the applicant is paying`
  * `housing:                  type of apartment: rented, owned, for free / no payment`
  * `existing_credits:         number of existing credits at this bank`
  * `job:                      current job information`
  * `num_dependents:           number of people being liable to provide maintenance`
  * `own_telephone:            is there any telephone registered for this customer?`
  * `foreign_worker:           is applicant foreign worker?`
  
We recommend the `skimr` (`skimr::skim`) and `DataExplorer` (`DataExplorer::plot_bar`, `DataExplorer::plot_histogram`, `DataExplorer::plot_boxplot`) packages as they create very well readable and understandable overviews.
We only use `DataExplorer` here; feel free to uncomment the `skimr` command.

```{r 09-use-cases-2-003}
# skimr::skim(german_credit)
```

```{r 09-use-cases-2-004, out.width="100%", fig.height=7}
DataExplorer::plot_bar(german_credit, nrow = 5, ncol = 3)
```

```{r 09-use-cases-2-005, out.width="100%", fig.height=4}
DataExplorer::plot_histogram(german_credit, nrow = 2, ncol = 3)
DataExplorer::plot_boxplot(german_credit, by = "credit_risk", nrow = 2, ncol = 3)
```

During this exploratory analysis meaningful analysis aspects could be:

  - Skewed distributions
  - Missing values
  - Empty / rare factor variables
  
However, in this use case we do not focus on the exploratory analysis.
  
### Modeling

The modeling process in `mlr3` can be split into five components:

1) The **task** defintion

2) The **learner** definition

3) The **training**

4) The **prediction**

5) The evaluation via **measures**

#### Task Definition

First of all, we are interested in the target which we want to model.
Most supervised machine learning problems are __regression__ or __classification__ problems.
However, note that other problems include unsupervised learning or time-to-event data (covered in `mlr3survival`).
In the `mlr3` context to distinguish between these problems, we define **tasks**.
If we want to solve a classification problem, we define a classifaction task -- `TaskClassif`.
For a regression problem, we establish a regression task -- `TaskRegr`.
Tasks are not only a different word for problems.
They also reflect some technical properties with respect to `mlr3`.

In our case it is clearly our objective to model or predict the binary `factor` variable `credit_risk`.
Thus, we use `TaskClassif`.

With `TaskClassif$new()` we can initialize a classification task. 
We need to specify a name for the task (we choose `"GermanCredit"`), the data we want to use (`credit`) and name of the target (`"credit_risk"`):

```{r 09-use-cases-2-007}
german_credit$credit_risk = as.factor(german_credit$credit_risk)
task = TaskClassif$new("GermanCredit", german_credit, "credit_risk")
```


#### Learner Definition

After having decided _what_ should be modeled, we need to decide on how.
This means we need to decide which learning algrorithms, or **Learners** are appropriate.
Using prior knowledge (e.g. knowing that it is a classification task or assuming that the classes are linearly separable) one ends up with one or some suitable **Learners**.
Many learners can be obtained via the `mlr3learners` package (CRAN). 
Additionally, many learners are provided on the `mlr3learners` organsisation [github page](https://github.com/mlr3learners).
These two ressources combined account for a large fraction of standard learning algorithms.
As `mlr3` usually only wraps learners from packages, it is easy to create a formal **Learner** by yourself using our template (). 
You may find the section 'Extending mlr3' in this book also very helpful.
If you happen to write your own **Learner** in `mlr3`, we would be happy if you share it with the `mlr3` community.

All available `Learner`s (i.e. all which you have installed from `mlr3`, `mlr3learners`, the mlr3learners github page or self-written ones) can be obtained by:

```{r 09-use-cases-2-008}
mlr_learners
```

For our problem, a suitable learner could be one of the following:
Logistic regression, CART, Random Forest (or many more).

A learner can be initialized with the `lrn` function and the name of the learner (e.g., `lrn("classif.xxx")`). 
Use `?mlr_learners_xxx` to open the help page of a learner named `xxx`, or use [the internet](https://mlr3learners.mlr-org.com/reference/index.html).

For example a logistic regression can be initialised in the following manner (logistic regression using R's `glm()` function and is provided by the `mlr3learners` package).

```{r 09-use-cases-2-009}
library(mlr3learners)
```

```{r 09-use-cases-2-010}
learner_logreg = lrn("classif.log_reg")
```

Alternatively but equivalently, one could use:

```{r 09-use-cases-2-011}
learner_logreg = LearnerClassifLogReg$new()
```

#### Training

Training is the procedure, where a model is fitted on data.
The selected model is trained so that is reflects the data situation in the best possible manner.

##### Logistic Regression

We start with the example of the logistic regression.
However, you will immediately see that the procedure generalises to any learner very easily.

An initialised learner can be trained on data using the `R6` method `train`.

```{r 09-use-cases-2-012}
learner_logreg$train(task)
```

Typically, in machine learning, one does not use the full data which is available but a subset, the so-called training data.

To efficiently perform a spilt of the data one could do the following:

```{r 09-use-cases-2-013}
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
```

80 percent of the data is used for training.
The remaining 20 percent are used for evaluation at a subsequent later point in time.
`train_set` is an integer vector displaying the selected rows of the original data set.

```{r 09-use-cases-2-014}
head(train_set)
```

In `mlr3` the training with a subset of the data is declared by the additional argument `row_ids = train_set`.

```{r 09-use-cases-2-015}
learner_logreg$train(task, row_ids = train_set)
```

We used logistic regression from the `stats` package.
To be precise `stats::glm` with the `family` argument set to `family = binomial()`.
All other arguments are set to the default of the `stats::glm` function.
`mlr3` only works as a wrapper. 
The original model can be accessed via:

```{r 09-use-cases-2-016, echo = FALSE}
learner_logreg$model
```

The stored object is a normal `glm` object and works with its `S3` methods.

```{r 09-use-cases-2-017}
class(learner_logreg$model)
summary(learner_logreg$model)
```

##### Random forest

Just like the logistic regression, we could train a random forest instead.
We use the fast implementation from the `ranger` package.
To do so we first need to define the learner and then actually train it.
Before we just used the default logistic regression learner.

We now additionally supply the importance argument (`importance = "permutation"`).
Doing so, we override the default and let the learner do feature importance determination based on permutation feature importance.

```{r 09-use-cases-2-018}
learner_rf = lrn("classif.ranger", importance = "permutation")
learner_rf$train(task)
```

We can access the importance values using `$importance()`.
The `R6` importance method plots the feature importance derived during training.

```{r 09-use-cases-2-019}
learner_rf$importance()
```

In order to obtain a better plot, we convert the importance into a `data.table`.

```{r 09-use-cases-2-020, out.width="100%", fig.height=4}
importance = as.data.table(learner_rf$importance(), keep.rownames = TRUE)
colnames(importance) = c("Feature", "Importance")
importance
ggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col() + coord_flip() + xlab("")
```

#### Prediction

After training the machine learning model, the model can be used for prediction.
Usually, prediction is the main purpose of machine learning models.
In our case, the model can be used to classify new credit applicants w.r.t. their associated credit status (good vs. bad) on the basis of the features.
Typically, machine learning models create numeric values.
In the regression case this is very natural.
For classification, most models create scores or probabilites.
Based on these values, one can derive class predictions.

#### Predict Classes

First, we directly predict classes.
Here, we do something which is almost never a good idea in machine learning:
Use the same data for prediction as for training.
However, for the purpose of demonstration we can negelct this.

```{r 09-use-cases-2-021}
pred_logreg = learner_logreg$predict_newdata(german_credit)
pred_rf = learner_rf$predict_newdata(german_credit)
pred_logreg
pred_rf
```

The `predict()` function gives a `Prediction` object. 
It can be converted to a `data.table` if one wants to use it downstream.

We display the prediction results aggregated in a confusion matrix.

```{r 09-use-cases-2-022}
pred_logreg$confusion
pred_rf$confusion
```

The random forest seems to perform much better than the logistic regression.
Based on this output, we would always want the random forest to do our credit worthiness classification.
However, as outlined, we evalaute on the training data.
It is possible that the results are steered by overfitting.
Additionally, we use the default options of the packages which seems 'unfair'.
It could be that the default options of the random forest are much better suited to the data situation than the logistic regression.
We should do performance evaluation, tuning and benchmarks

## Predict Probabilities

Most learners may not only predict a class variable ("response"), but also their degree of "belief" / uncertainty in a given response.
Typically, we achieve this by setting the `$predict_type` slot to `"prob"`.
Sometimes this needs to be done *before* the learner is trained.
This is necessary because the wrapped package requires this.
Alternatively, we can directly create the learner with this option: `lrn("classif.log_reg", predict_type = "prob")`

```{r 09-use-cases-2-023}
learner_logreg$predict_type = "prob"
```

```{r 09-use-cases-2-024}
learner_logreg$predict_newdata(german_credit)
```

Note that sometimes one needs to be cautious when dealing with the probability interpretation of the predictions.

#### Performance Evaluation

To measure the performance of a learner on new unseen data, we usually mimic the scenario of unseen data by splitting up the data into training and test set.
The training set is used for training the learner, and the test set is only used for predicting and evaluating the performance of the trained learner.
Numerous resampling methods (cross-validation, bootstrap) repeat the splitting process in different ways.

We need to specify the resampling strategy using the `rsmp()` function:

```{r 09-use-cases-2-025}
resampling = rsmp("holdout", ratio = 2/3)
print(resampling)
```

Here, we use "holdout", a simple train-test split (with just one interation).
We use the `resample()` function to undertake the resampling calculation:

```{r 09-use-cases-2-026}
res = resample(task, learner_logreg, resampling)
res
```

The default score of the measure is included in the `$aggregate()` slot:

```{r 09-use-cases-2-027}
res$aggregate()
```

The default in this scenario is `classif.ce`. 
This refers to the classification error. 
A low value close to one is desired.

We can easily run differend resampling strategies, e.g. repeated holdout (`"subsampling"`), or cross validation.
Most methods perform repeated train/predict cycles on different data subsets and aggregate the result (usually as the `mean()`). 
Doing this manually would require us to write loops.
`mlr3` does the job for us.

```{r 09-use-cases-2-028}
res_sub = resample(task, learner_logreg, rsmp("subsampling", repeats = 10))
res_sub$aggregate()
```

Instead, we could also apply cross-validation.

```{r 09-use-cases-2-029}
res_cv = resample(task, learner_logreg, rsmp("cv", folds = 10))
res_cv$aggregate()
```

`mlr3` features scores for many more measures. 
Here we apply `msr("classif.fpr")` for the false positive rate, or `msr("classif.fnr")` for the false negative rate.
Multiple measures are entered via a list.

```{r 09-use-cases-2-030}
# false positive rate
res_cv$aggregate(msr("classif.fpr"))
# false positive rate and false negative
msr_list = list(
  msr("classif.fpr"),
  msr("classif.fnr")
)
res_cv$aggregate(msr_list)
```

There are a few more resampling methods, and quite a few more measures. 
We list them in

```{r 09-use-cases-2-031}
mlr_resamplings
```

and 

```{r 09-use-cases-2-032}
mlr_measures
```

To get help on a resampling method, use `?mlr_resamplings_xxx`, for a measure do `?mlr_measures_xxx`. 
You can also use the [mlr3 reference](https://mlr3.mlr-org.com/reference/index.html) online.

Some measure, for example `"auc"`, require a "probability" prediction, instead of a response prediction.

#### Performance Comparison and Benchmarks

We could compare `Learners` by evaluating `resample()` for each of them manually.
However, `benchmark()` automatically performs resampling evaluations for multiple learners and tasks.
`benchmark_grid()` creates fully crossed designs:
Multiple `Learner`s for multiple `Task`s are compared w.r.t. multiple `Resampling`s.
```{r 09-use-cases-2-033}
lrn_list = list(
  lrn("classif.log_reg", predict_type = "prob"),
  lrn("classif.ranger", predict_type = "prob")
)
bm_design = benchmark_grid(task = task, resamplings = rsmp("cv", folds = 10), learners = lrn_list)
```

At this point, we want to mention that large benchmarks will (naturally) be very time-consuming and computionally expensvie.
Our use case is very simple, so expect it to take about one minute.
In general, we want use *parallelization* to speed things up on multicore machines.
A benchmark is an 'embarassingly parallel' task, so in this case it would be very easy.
This does **not work** on rstudio cloud, though.
So only uncomment it locally!

```{r 09-use-cases-2-034}
# future::plan("multiprocess")
```

```{r 09-use-cases-2-035} 
bmr = benchmark(bm_design)
```

In the benchmark we can compare different measures. 
We compare misclassification rate and AUC.

```{r 09-use-cases-2-036}
msr_list = list(msr("classif.ce"), msr("classif.auc"))
performances = bmr$aggregate(msr_list)
performances[, c("learner_id", "classif.ce", "classif.auc")]
```

In contrast to before, we see that the two models perform very similarly.
The perfect class assignment from before was obviously a very optimistic evalution.

### Deviating from package defaults

The previously shown techniques build the backbone of an `mlr3`-featured machine learning work bench.
However, in most cases one would never proceed in the way we did.
While many `R` packages have carefully selected default settings, they will not perform optimally in any scenario.
Typically, we can select the values of hyperparameters.
These (hyper)parameters can be accessed via

```{r 09-use-cases-2-037}
learner_logreg$param_set$get_values()
```

or 

```{r 09-use-cases-2-038}
learner_logreg$param_set
```

As of now, the parameter set is empty.

We can parametrise our learners in two distinct manners.
If we have prior knowledge on how the learner should be (hyper-)parametrised, the way to go would be manually entering the parameters in the parameter set.
In most cases, however, we would want to tune the model so that it can search good model configurations itself.
For now however we only want to compare a small number of models.

To get an idea on which parameters can be manipulated, we can investigate the parameters of the original package version.

```{r 09-use-cases-2-039}
?ranger::ranger
```

```{r 09-use-cases-2-040, echo= FALSE}
expression1 = floor(sqrt(ncol(german_credit) - 1))
``` 

For the random forest two meaninful parameters which steer model complexity are:
`num.trees` and `mtry`

`num.trees` defaults to `500` and `mtry` to `floor(sqrt(ncol(data) - 1))`, in our case `r expression1`.

In the following we aim to train three different learners:

1) The default random forest.

2) A random forest with low `num.trees` and low `mtry`.

3) A random forest with high `num.trees` and high `mtry`.

We will benchmark their performance on the German credit data set.

We construct the three learners and set the parameters with a named list.

```{r 09-use-cases-2-041}
learner_rf_medium = lrn("classif.ranger", predict_type = "prob")

learner_rf_low = lrn("classif.ranger", predict_type = "prob")
learner_rf_low$param_set$values <- list(num.trees = 5, mtry = 2)

learner_rf_high = lrn("classif.ranger", predict_type = "prob")
learner_rf_high$param_set$values <- list(num.trees = 1000, mtry = 11)
```

Once the learners are defined, we can benchmark them.

```{r 09-use-cases-2-042}
lrn_list = list(
  learner_rf_low = learner_rf_low,
  learner_rf_medium = learner_rf_medium,
  learner_rf_high = learner_rf_high
)
bm_design = benchmark_grid(task = task, resamplings = rsmp("cv", folds = 10), learners = lrn_list)
```

```{r 09-use-cases-2-043}
bmr = benchmark(bm_design)
```

We compare misclassification rate and AUC again.

```{r 09-use-cases-2-044}
msr_list = list(msr("classif.ce"), msr("classif.auc"))
performances = bmr$aggregate(msr_list)
cbind(learner = names(lrn_list), performances[, c("classif.ce", "classif.auc")])
```

All learners seem to have similar performance while the `learner_rf_low` seems to underfit a bit.
